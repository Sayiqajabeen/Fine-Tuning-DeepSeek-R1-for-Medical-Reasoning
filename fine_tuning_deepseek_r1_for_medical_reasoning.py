# -*- coding: utf-8 -*-
"""Fine-Tuning DeepSeek-R1 for Medical Reasoning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qJ1TVHYmrOjgL1WOan-0qsQ3Aw0SBP8O
"""

# install relevant packages

# Install the Unsloth library using pip
!pip install unsloth
# Reinstall Unsloth from GitHub with fresh files
!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git

# Explanation of flags used in the second command:
# --force-reinstall   → Removes the old version and installs a fresh copy.
# --no-cache-dir      → Prevents using old cached files, ensuring a clean install.
# --no-deps           → Skips installing extra dependencies (only installs Unsloth itself).
# git+https://...     → Downloads and installs the latest version directly from GitHub.

!pip install unsloth

!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git

#import all relevant libraries
from unsloth import FastLanguageModel
import torch
from trl import SFTTrainer
from unsloth import is_bfloat16_supported

from huggingface_hub import login
from transformers import TrainingArguments
from datasets import load_dataset

import wandb
# from kaggle_secrets import UserSecretsClient

"""**Intialize huggingface and wandb tokens**"""

import wandb
from huggingface_hub import login  # Make sure this is installed: pip install huggingface_hub

# Option 1: Enter your secrets manually (recommended for Colab)
hugging_face_token = input("Enter your Hugging Face token: ")
wnb_token = input("Enter your Weights & Biases token: ")

# Option 2: Upload a JSON file with tokens (if you prefer not typing them each time)
# from google.colab import files
# uploaded = files.upload()  # Upload a file like `secrets.json` with keys "hugging_face" and "wnb"
# import json
# with open("secrets.json") as f:
#     secrets = json.load(f)
# hugging_face_token = secrets["hugging_face"]
# wnb_token = secrets["wnb"]

# Login to Hugging Face and W&B
login(hugging_face_token)
wandb.login(key=wnb_token)

# Initialize W&B run
run = wandb.init(
    project='Fine-tune-DeepSeek-R1',
    job_type="training",
    anonymous="allow"
)

#5b4b47ce1ec09902bfc8e341294ed96bde73e13c
#hf_tok=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

import os

# Disable Flash Attention, Triton-based ops
os.environ["FLASH_ATTENTION_FORCE_DISABLED"] = "1"
os.environ["DISABLE_TRITON"] = "1"
os.environ["USE_FLASH_ATTENTION"] = "0"

# key parameters
max_seq_length = 2048
dtype = None
load_in_4bit = True

#load the Deepseek R1 model and tokenizer using unsolth
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/DeepSeek-R1-Distill-Llama-8B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    token = hugging_face_token,
)



"""**Test the Model before finetune**"""

prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.
Please answer the following medical question.

### Question:
{}

### Response:
<think>{}"""

question = """A 50-year-old man with a history of chronic alcohol use presents with confusion, ataxia,
              and ophthalmoplegia. His wife reports that he has been forgetful
              and has difficulty recalling recent events. What is the most likely diagnosis,
              and what immediate treatment should be administered?"""

#Enable Optimized inference mode for unsloth models
FastLanguageModel.for_inference(model)

# Format the question using the structured prompt
inputs = tokenizer([prompt_style.format(question, "")], return_tensors = "pt").to("cuda")

#Generate the response using the model
outputs = model.generate(
    input_ids = inputs.input_ids,
    attention_mask = inputs.attention_mask,
    max_new_tokens = 1200,
    use_cache = True
)

#Decode the output response in the readable text
response = tokenizer.batch_decode(outputs)

#Show and print only relevant response
print(response[0].split("### Response:")[1])

"""**Finetune Step by Step**"""

# step 1:
train_prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.
Please answer the following medical question.

### Question:
{}

### Response:
<think>
{}
</think>
{}"""

# step 2: Download the dataset using huggingface and format it for tuning
dataset = load_dataset("FreedomIntelligence/medical-o1-reasoning-SFT", "en", split = "train[0:1000]", trust_remote_code = True)

dataset

dataset[0]

# we need to format the dataset to fit our prompt style
end_of_sentence_token = tokenizer.eos_token
end_of_sentence_token

#formitting prompt function
def formatting_prompts_func(examples):
    inputs = examples["Question"]
    cots = examples["Complex_CoT"]
    outputs = examples["Response"]
    texts = []
    for input, cot, output in zip(inputs, cots, outputs):
        text = train_prompt_style.format(input, cot, output) + end_of_sentence_token
        texts.append(text)
    return {
        "text": texts,
    }

# update dataset formatting
dataset_finetune = dataset.map(formatting_prompts_func, batched = True,)
dataset_finetune["text"][0]

model_lora = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)

!pip install -U bitsandbytes triton

# Initialize the finetune trainer
trainer = SFTTrainer(
    model = model_lora,
    tokenizer = tokenizer,
    train_dataset = dataset_finetune,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 1,  # Reduced from 2 to lower memory usage

    #training arguments
     args=TrainingArguments(
        per_device_train_batch_size=1,  # Reduced from 2
        gradient_accumulation_steps=8,  # Increased from 4 to compensate for smaller batch size
        # Use num_train_epochs = 1, warmup_ratio for full training runs!
        warmup_steps=5,
        max_steps=60,
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=10,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
        # Add gradient checkpointing to save memory
        gradient_checkpointing=True,
    ),
    # Add packing option to False if you're having issues
    packing=False,
)

# Run training with explicit error catching
try:
    trainer_stats = trainer.train()
except RuntimeError as e:
    print(f"Error details: {e}")
    # If it's CUDA OOM, you might see this in the error
    if "CUDA out of memory" in str(e):
        print("This is a CUDA memory error. Try further reducing batch size or model size.")
    elif "PassManager::run failed" in str(e):
        print("This might be related to PyTorch/CUDA optimization passes failing.")

import wandb
# Save the fine-tuned model
wandb.finish()

question = "A 25-year-old woman presents with unintentional weight loss, heat intolerance, and frequent episodes of palpitations. On examination, she has a diffusely enlarged, non-tender thyroid gland and exophthalmos. Her laboratory results show low TSH and elevated free T3 and T4. What is the most likely diagnosis, and what are the treatment options?"


FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=1200,
    use_cache=True,
)
response = tokenizer.batch_decode(outputs)
print(response[0].split("### Response:")[1])

question = "A 60-year-old man with a history of smoking presents with persistent cough, hemoptysis, and unintentional weight loss. Imaging reveals a lung mass, and laboratory tests show hypercalcemia. What is the most likely underlying cause of his hypercalcemia, and what is the probable diagnosis?"

inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=1200,
    use_cache=True,
)
response = tokenizer.batch_decode(outputs)
print(response[0].split("### Response:")[1])

new_model_online = "kingabzpro/DeepSeek-R1-Medical-COT"
new_model_local = "DeepSeek-R1-Medical-COT"
model.save_pretrained(new_model_local) # Local saving
tokenizer.save_pretrained(new_model_local)

